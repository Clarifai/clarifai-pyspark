{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09934f97-509c-4fa7-ab25-a674835ff57a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c70a237e-3168-4bd5-b7e9-fe49da3eeb75",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Installing the required python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d33eebd-4547-4055-a704-17631456a6b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install clarifai-pyspark\n",
    "!pip install protobuf==4.24.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b822de8e-783a-4aa5-94bc-3a454befb1a5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Setting CLARIFAI_PAT as an environment variable.  \n",
    "*Note: Guide to get your [PAT](https://docs.clarifai.com/clarifai-basics/authentication/personal-access-tokens)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6ea3ecb-624d-40a6-b8b5-72aac4765580",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from clarifaipyspark.client import ClarifaiPySpark\n",
    "#Mention your PAT\n",
    "os.environ['CLARIFAI_PAT'] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1926f48-e305-416c-8d33-b0b07143482f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Clarifai-PySpark Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f048b9a5-0f6c-4a57-bf6e-d4c513c52d10",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Create a ClarifaiPyspark client object to connect to your App on Clarifai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ff59442-b953-47e3-b50c-0eb11a8276f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cspark_obj = ClarifaiPySpark(user_id='user_id', app_id='app_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eacc6ee9-7efb-48b6-9eb7-515ea8d01816",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Mention the dataset from your App on which you want to work.  \n",
    "This creates a new dataset in the App if it doesn't already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc9d473d-3f61-41d0-ab65-17c7dab8b8b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset_obj = cspark_obj.dataset(dataset_id='dataset_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6865d686-e89f-4b46-982e-138302bf8e69",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 1. Ingest Dataset from Databricks Volume to Clarifai App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8037987-2191-414a-b3ec-d4ac66e80094",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### a. Upload from Volume folder   \n",
    "If your dataset images or text files are stored within a Databricks volume, you have the option to directly upload the data files from the volume to your Clarifai App.   \n",
    "Please ensure that the folder solely contains images/text files. If the folder name serves as the label for all the images within it, you can set labels parameter to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39e4ae83-291e-4273-a422-750cda2d8039",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset_obj.upload_dataset_from_folder(folder_path='volume_folder_path', \n",
    "                                       input_type='image/text', \n",
    "                                       labels=True/False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d0a0fd6-ccc5-461a-aa7b-b8d8594ba9b4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### b. Upload from CSV\n",
    "You can populate the dataset from a CSV that must include these essential columns: 'inputid' and 'input'.   \n",
    "Additional supported columns in the CSV are 'concepts', 'metadata', and 'geopoints'.   \n",
    "The 'input' column can contain a file URL or path, or it can have raw text. If the 'concepts' column exists in the CSV, make sure to set 'labels=True'.   \n",
    "You also have the option to use a CSV file directly from your AWS S3 bucket. Simply specify the 'source' parameter as 's3' in such cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b48df9a5-51ce-4392-a4ac-1cb749a18586",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset_obj.upload_dataset_from_csv(csv_path='volume_csv_path/S3_csv_path', \n",
    "                                    input_type='text/image', \n",
    "                                    labels=True/False, \n",
    "                                    csv_type='raw/url/filepath',\n",
    "                                    source='volume/s3')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0196f0ba-81a3-40a0-b294-9fe4e52decfb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### c. Upload from Delta table\n",
    "You can employ a delta table to populate a dataset in your App.   \n",
    "The table should include these essential columns: 'inputid' and 'input'.   \n",
    "Furthermore, the delta table supports additional columns such as 'concepts,' 'metadata,' and 'geopoints.'   \n",
    "The 'input' column is versatile, allowing it to contain file URLs or paths as well as raw text.   \n",
    "If the 'concepts' column is present in the table, remember to enable the 'labels' parameter by setting it to 'True.'   \n",
    "You also have the choice to use a delta table stored within your AWS S3 bucket by providing its S3 path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "233589f9-55cf-4862-8c38-b633db8b9509",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset_obj.upload_dataset_from_table(table_path='volume_table_path', \n",
    "                                      input_type='text/image', \n",
    "                                      labels=True/False, \n",
    "                                      csv_type='raw/url/filepath')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "377caad0-6b82-40db-be52-80a0316b5144",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### d. Upload from Dataframe\n",
    "You can upload a dataset from a dataframe that should include these required columns: 'inputid' and 'input'.   \n",
    "Additionally, the dataframe supports other columns such as 'concepts', 'metadata', and 'geopoints.'   \n",
    "The 'input' column can accommodate file URLs or paths, or it can hold raw text.   \n",
    "If the dataframe contains the 'concepts' column, ensure to set 'labels=True'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b81db38-0dab-4b34-8b0e-64241021dca2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset_obj.upload_dataset_from_table(dataframe=spark_dataframe, \n",
    "                                      input_type='text/image', \n",
    "                                      labels=True/False, \n",
    "                                      csv_type='raw/url/filepath')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6efdce4d-01c7-4cb5-b595-632c6598469f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### e. Upload with custom dataloader\n",
    "In case your dataset is stored in an alternative format or requires preprocessing, you have the flexibility to supply a custom dataloader.   \n",
    "You can explore various dataloader examples for reference [here](https://github.com/Clarifai/examples/tree/main/datasets/upload).   \n",
    "The required files & folders for dataloader should be stored in databricks volume storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac853504-9dcb-4d59-9d30-4b0110e812ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset_obj.upload_dataset_from_dataloader(task=\"visual-classification\", \n",
    "                                           split=\"train\", \n",
    "                                           module_dir=\"volume_module_path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7d09fb0-99d5-4f6e-955e-e5d0a02d0335",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 2. Fetching Dataset Information from Clarifai App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7e7f597-6be7-41ff-a20c-624d31bfc509",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### a. Retrieve data file details in JSON format\n",
    "To access information about the data files within your Clarifai App's dataset, you can use the following function which returns a JSON response.   \n",
    "You may use the 'input_type' parameter for retrieving the details for a specific type of data file such as 'image', 'video', 'audio', or 'text'.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1a6c44c-c8d2-4539-9f62-aad199fa2c39",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "inputs_response = list(dataset_obj.list_inputs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Retrieve data file details as a dataframe\n",
    "You can also obtain input details in a structured dataframe format, featuring columns such as 'input_id,' 'image_url/text_url,' 'image_info/text_info,' 'input_created_at,' and 'input_modified_at.'   \n",
    "Be sure to specify the 'input_type' when using this function.   \n",
    "Please note that the the JSON response might include additional attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_df = dataset_obj.export_inputs_to_dataframe(input_type=\"text/image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ded51252-a892-48da-b819-cc1f0f058406",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### b. Download image/text files from Clarifai App to Databricks Volume\n",
    "With this function, you can directly download the image/text files from your Clarifai App's dataset to your Databricks volume.   \n",
    "You'll need to specify the storage path in the volume for the download and use the response obtained from list_inputs() as the parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39c7c78d-4043-457a-a9d4-bdf18a78ee32",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#For images\n",
    "dataset_obj.export_images_to_volume(path=\"destination_volume_path\", \n",
    "                                    input_response=inputs_response)\n",
    "\n",
    "#For text\n",
    "dataset_obj.export_text_to_volume(path=\"destination_volume_path\", \n",
    "                                  input_response=inputs_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac7c35e4-9090-492e-b4af-9f118fb5d434",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 3. Fetching Annotations from Clarifai App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Retrieve annotation details in JSON format\n",
    "To obtain annotations within your Clarifai App's dataset, you can utilize the following function, which provides a JSON response.   \n",
    "Additionally, you have the option to specify a list of input IDs for which you require annotations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_response = list(dataset_obj.list_annotations(input_ids=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Retrieve annotation details as a dataframe\n",
    "You can also acquire annotations in a structured dataframe format, including columns like annotation_id’, 'annotation', 'annotation_user_id', 'iinput_id', 'annotation_created_at' and ‘annotation_modified_at’.   \n",
    "If necessary, you can specify a list of input IDs for which you require annotations.   \n",
    "Please note that the JSON response may contain supplementary attributes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_df = dataset_obj.export_annotations_to_dataframe(input_ids=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Acquire inputs with their associated annotations in a dataframe\n",
    "You have the capability to retrieve both input details and their corresponding annotations simultaneously using the following function.   \n",
    "This function produces a dataframe that consolidates data from both the annotations and inputs dataframes, as described in the functions mentioned earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0fb331b6-c5af-4422-9223-3f87ace21a90",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset_df = dataset_obj.export_dataset_to_dataframe()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ClarifaiPyspark Example NB",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
